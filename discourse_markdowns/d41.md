So the questions expecting JSON will be jsonified separately before passing to the evaluator because in the current implementation in the text field idk why it is failing to load the json with \" however I could load the exact thing using json.loads in python.
hi @carlton @Jivraj ! could you please provide us with the correct answers for all the 57 GA questions, so that it would be really helpful for us to cross check if our app is returning the correct answer for each question. please consider sharing the correct answers for all the questions as they are not available in the seek portal as well
Hi @23f2003413 We won’t be sending codes for any of the questions, regarding validation part you can submit answer to portal and if that works. All first 5 GA’s were conducted on anand sir’s proxy server portal,  so just enable check answers button and you can test answers.
can u please let me know on how to enable the check answers button? and which proxy server portal are you talking about. please help me out!
@Jivraj @carlton sir 1000145015 1080×2340 234 KB For questions like this (there’s a similar one in GA1 too, will the user send the USER and REPO of their GitHub account as parameters too? Even if they do, my script may not get necessary authentication to create repo and make commit in their repository. In this case, would I have to implement GitHub OAuth flow? If they don’t send those details but just the email and I am expected to just change the email in my own repo and commit, in the worst case making the same change repeatedly may be misconstrued as a DDoS attack by an automated script which may lead to my GitHub account being suspended, which does not seem ideal. Can you please at least hint at a solution? Edit: Same query for the GitHub actions question (GA2 Q7) Another edit: A similar query for GA2 Q8. Repeated dockerimage pushes to Dockerhub with different tags.
The image shows instructions for publishing a webpage on GitHub Pages.  Users must include their email address, '22f3000819@ds.study.iitm.ac.in', in the HTML, wrapped in a specific HTML comment for email obfuscation. The user is then prompted to enter the URL of the GitHub Pages site, formatted as "[USER].github.io/[REPO]/". A cache busting URL parameter is also mentioned.
Use the following script to enable answer boxes and check answers buttons: inputs = document.querySelectorAll('input')
textboxes = document.querySelectorAll("textarea")
buttons  = document.querySelectorAll("button")
inputs.forEach(input => input.removeAttribute('disabled'));
buttons.forEach(input => input.removeAttribute('disabled'));
textboxes.forEach(input => input.removeAttribute('disabled')); This was provided with the Mock ROE2 mail.
For GA-1 question 10, we need to get the hash of the json, from the site JSON Hash , I found that underneath it using sha-256 in a function inside the encrypt.js file but still when I implemented the same in python, it is giving a different hash. Can we get the hashing code in python? @carlton @Jivraj Update: I was successful in getting the hash in js.
You won’t be required to update someone else’s repo. Any github name, repo name is fine. Just that it should have github pages with that particular email address. 22f3000819: Same query for the GitHub actions question (GA2 Q7) Same for GA2 Quetion7 as well, you github url is required. 22f3000819: If they don’t send those details but just the email and I am expected to just change the email in my own repo and commit, in the worst case making the same change repeatedly may be misconstrued as a DDoS attack by an automated script which may lead to my GitHub account being suspended, which does not seem ideal. Can you please at least hint at a solution? In evaluation we will only send request one worst case twice or thrice if it fails from our end, so no issues with that.
The image shows a white, sans-serif letter "S" against a solid, bright blue background. The "S" appears slightly blurred or out of focus, creating a soft edge. The composition is simple, focusing solely on the contrast between the letter and the background color.

The image features the letter "S" in a white, slightly blurred font. It is centered against a solid, bright blue background. The simplicity of the image suggests it could be a logo element or a simple illustration.

21f2000709: I was successful in getting the hash in js. I was thinking of same solution it would definetly work in js.
Here's a 50-word description of the image:

The image shows a person with short, dark hair and a light complexion. The person is wearing a dark, patterned shirt with a plaid or checkered design.  The background is slightly blurred, featuring some greenery, possibly plants or trees, suggesting an outdoor or indoor setting with foliage.

Okay. Thank you sir. I’ll just do this after the rest for now. Another question .. In GA1 Q6, will the hidden element be sent as an html file or will the tag be included in the question parameter?
@carlton and @jivraj , Hello.. I am facing issue with zip file extraction function which is in many GA1 questions. e.g., for GA1 Q15, if I use 7-zip to extract files, it shows the correct total size when using Postman for local server. If I use python built-in function, it calculates a different value (which is incorrect). So, I used 7-zip. But I (with help from copilot agent) am not able to make 7-zip work on the vercel deployment. Can we just hard code the answers to GA1 questions, or is there a way to take answer from local server and use it for response in vercel (sorry, it doesn’t even make sense to me, but a lot of things are hard to understand anyway)? Or how do we resolve this 7-zip issue. Python built-in extraction did not work correctly even after hours of trying.. I have another question. Due to lack of time, if I just submit the Project 2, with question/answers to GA1, (and if these all work as expected on vercel), how much approx score can we get if I am not able to do other GAs at all for Project 2. Edit after posting: I can hard code questions and other answers from GA2 upto GA4 (I missed GA5, so don’t have those answers). Thank you.
I too want to know like by 5 questions at random, is it 1 random question per GA or 5 random questions from the entire 5 GAs? In the later case, there would be no certainty unless all of our questions works and there aren’t any unexpected surprises at the evaluation.
in the GA 4 question 5 ( Find the bounding box of a city ) Will the OSM_id ending digits be provided. In the question body they are not there but on submission they come like in the attached screenshot. image 1614×372 45 KB
The image highlights the impact of UrbanRide's automation of bounding box data, leading to optimized routing, improved fleet allocation, enhanced market analysis, and scalable operations.  It then poses a question about the minimum latitude of Tianjin's bounding box using the Nominatim API, but the provided answer "38.566" triggers an error, indicating an incorrect latitude value and suggesting a check of the OSM ID ending with 2077.

github.com GitHub - Tusharisme/TDS_Project_2 Contribute to Tusharisme/TDS_Project_2 development by creating an account on GitHub. This is my project github repo link.I just wanted your guys suggestion with this…like am I going correct @carlton @Jivraj @s.anand
The image shows a GitHub repository named "Tusharisme/TDS_Project_2". It has one contributor, zero issues, stars, or forks. An icon to the right of the repository name displays a stylized, pixelated design with blue and white squares. The bottom of the image shows the GitHub logo.
I faced the same issue. Initially, I used geopy.geocoders to solve the question, and it provided the correct answer during the assignment submission. However, the same approach is now giving an incorrect result. Instead of using geopy , try using this URL directly: https://nominatim.openstreetmap.org/search . This worked for me.
I have a doubt like when I pass answers in string and in the questions about markdown will the \n characters be properly parsed before checking the correctness of the markdown or will it be directly checked for valid markdown? Because the raw string when pasted in the text box of the GA isn’t getting the markdown. In case of image compression question, should I return the base64 encoding of the compressed image? @Jivraj
@carlton @Jivraj @Saransh_Saini Respected TDS Team, I currently have Three doubts regarding Project 2 : As per the 19th March session, it was mentioned that the files would be the same for everyone, and only the parameters in the questions would vary. However, I have noticed that the files can actually be different. To support this, I’m attaching screenshots of the CSV files for GA5 Q1—one is mine, and the other belongs to a batchmate. mydata 939×401 18.3 KB friend data 938×465 20.5 KB During the session, it was said that uploading files would take time, and the suggested solution was to pre-download the files on the server since they are supposed to be the same. But since the files are not identical for all students, this issue needs to be addressed. In GA4 Q2 , my task is to retrieve movie information from IMDb for all films with a rating between 3 and 5. I am scraping the correct movie names(for example 6th movie in given image), but the portal is accepting them differently. All movie names are provided in English, but the portal seems to be accepting some titles in other languages—Spanish, Dutch, I believe. image 1674×420 24.3 KB image 1118×358 63.7 KB Please check the issue in the images provided. How to handle this question. In GA4 Q10 , very few students were able to solve the question using LLMs or Python during the assignment. Most of us ended up solving it manually. At that time, @carlton sir had mentioned that the question would be revised. Here is the thread link for reference. How should we handle this question now? Thankyou
This table displays sales data, including Transaction IDs, customer names, countries, dates, product codes, sales amounts in USD, and costs in USD.  It contains data for various customers like John Doe, Frank Thomas, and Bob Brown, spanning across different countries such as France, United States, and the United Arab Emirates. Dates range from 2022 to 2023.
This image shows a table of transaction data. It includes transaction IDs, customer names, countries, dates, product codes, sales, and costs in USD.  Various customers from different countries like IND, US, UK, AE, and FRA are listed with their corresponding transactions. John Doe appears to have the most transactions listed.
The image shows a JSON data input with an error. The JSON data includes an "id," "title," "year," and "rating" for a movie. The error indicates a mismatch in the "title" value; the expected value and the actual value are slightly different. The text also mentions that IMDb search results might vary by region.
"Nadaaniyan," set to release in 2025, is about a privileged Delhi socialite who hires a middle-class student to act as her boyfriend. This pretense to maintain her social status becomes complicated as genuine feelings emerge between the two. The show has a rating of 3.0 and is rated TV-14.
// GA1 question 9:
    //     curl -X POST "http://localhost:8000/api/" -H "Content-Type: multipart/form-data" -F "question=Sort this JSON array of objects by the value of the age field. In case of a tie, sort by the name field. Paste the resulting JSON below without any spaces or newlines. 
    // [{\"name\":\"Alice\",\"age\":92},{\"name\":\"Bob\",\"age\":28},{\"name\":\"Charlie\",\"age\":16},{\"name\":\"David\",\"age\":56},{\"name\":\"Emma\",\"age\":70},{\"name\":\"Frank\",\"age\":67},{\"name\":\"Grace\",\"age\":36},{\"name\":\"Henry\",\"age\":94},{\"name\":\"Ivy\",\"age\":44},{\"name\":\"Jack\",\"age\":53},{\"name\":\"Karen\",\"age\":65},{\"name\":\"Liam\",\"age\":23},{\"name\":\"Mary\",\"age\":97},{\"name\":\"Nora\",\"age\":68},{\"name\":\"Oscar\",\"age\":57},{\"name\":\"Paul\",\"age\":88}]"
    {
        "answer": "[{\"name\":\"Charlie\",\"age\":16},{\"name\":\"Liam\",\"age\":23},{\"name\":\"Bob\",\"age\":28},{\"name\":\"Grace\",\"age\":36},{\"name\":\"Ivy\",\"age\":44},{\"name\":\"Jack\",\"age\":53},{\"name\":\"David\",\"age\":56},{\"name\":\"Oscar\",\"age\":57},{\"name\":\"Karen\",\"age\":65},{\"name\":\"Frank\",\"age\":67},{\"name\":\"Nora\",\"age\":68},{\"name\":\"Emma\",\"age\":70},{\"name\":\"Paul\",\"age\":88},{\"name\":\"Alice\",\"age\":92},{\"name\":\"Henry\",\"age\":94},{\"name\":\"Mary\",\"age\":97}]"
    } Is it ok for GA 1 Question 9 answer output to look like this because it matches with the answer just it has the extra back slash…What should i do def sort_json_array(json_array: str, sort_keys: list) -> str:
    """
    Sort a JSON array based on specified criteria

    Args:
        json_array: JSON array as a string
        sort_keys: List of keys to sort by

    Returns:
        Sorted JSON array as a string
    """
    try:
        # Parse the JSON array
        data = json.loads(json_array)

        # Sort the data based on the specified keys
        for key in reversed(sort_keys):
            data = sorted(data, key=lambda x: x.get(key, ""))

        # Return the sorted JSON as a string without whitespace
        return json.dumps(data, separators=(",", ":"))

    except Exception as e:
        return f"Error sorting JSON array: {str(e)}" {
            "type": "function",
            "function": {
                "name": "sort_json_array",
                "description": "Sort a JSON array based on specified criteria",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "json_array": {
                            "type": "string",
                            "description": "JSON array to sort",
                        },
                        "sort_keys": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of keys to sort by",
                        },
                    },
                    "required": ["json_array", "sort_keys"],
                },
            },
        }, @carlton @Jivraj
Image URL 'https://europe1.discourse-cdn.com/flex013/uploads/iitm/original/3X/2/e/2e577720e007a687c9e3aeecd301efb7b64eb222.gif' has an unsupported MIME type ('image/gif'). Skipping explanation.
Hi @23f2003751 Short Answer: No Long Answer: Backslashes are usually not a problem when converting a string to JSON. But somehow the evaluation script isn’t taking this into consideration. Check this out. This is my response image 1113×125 3.01 KB which is shown incorrect just after adding 2 backslashes image 1107×122 4.01 KB Note: It would be better to check your responses on the TDS portal before finalising the script. You can access the answer box and check button by removing the disable attribute.
The image shows a "Sorted JSON" with a list of dictionaries. Each dictionary contains "name" and "age" key-value pairs, representing people like Bob (11), Emma (11), and Grace (14). A fourth element shows "na", and a checkmark indicates the JSON is sorted correctly. The text "Correct" at the bottom confirms the validation.

The image shows a code snippet labeled "Sorted JSON" containing an array of objects with "name" and "age" properties. However, the JSON is invalid, resulting in a `SyntaxError: Expected property name or '}' in JSON at position 16`. The error seems to stem from an incomplete object `{ "r` at the end of the JSON string.
@Saransh_Saini it can’t be fixed actually, you would need the answer in the json format {"answer": "actual_answer"} but if the actual_answer needs to be string and if it happens to be of json type, it has to be represented using \"
