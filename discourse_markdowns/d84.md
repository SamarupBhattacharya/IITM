@carlton @Jivraj I wanted to kindly request if you could review the bonus additional tasks, as they were not reflected in the evaluation, despite being mentioned in the instructions. Apart from that I understand and accept my score overall, especially since I had hardcoded the folder paths in my prompt for some questions, which I believe led to those failures. Bonus: Additional tasks . We may pass additional tasks beyond the list above. If your code handles them correctly, you get 1 bonus mark per task. Regards,
Would you mind reviewing the evaluation.log screenshot I have attached? I believe I may deserve marks for Task B6. @carlton , could you kindly take a look? image 1460×585 24.9 KB
The image shows an HTTP request to retrieve data from `/data/b6.json`. The expected result is a list of names, but the actual result is a JSON object with a `.author` field, which contains the list of names. The retrieved data doesn't match the expected data, causing the "B6" test to fail.
I am also facing the same Please help my roll no is 21f3001750
can you please take a look at this screenshot? image 1451×640 64.9 KB The task was done but the LLM made a mistake. I think this type of mistake was outside our control. @carlton
The image shows a task to extract a credit card number from an image (`/data/card.jpg`) using an LLM and save it to `/data/cc-number.txt`. The POST request indicates the task description. While the server returns a 200 OK, the extracted card number in `/data/cc-number.txt` doesn't fully match the expected value. So the process is completed but the result is incorrect.
@carlton @Jivraj Please correct me if I’m wrong, but I noticed that for tasks B7 , B8 , and B10 , the evaluation log does not include any POST or GET request traces , unlike the other tasks which have clearly recorded request flows, generated code, and outputs. In these three cases, the log shows only the failure message without any indication that the script was executed or that the output file was read. image 2003×745 95 KB
The image shows a series of HTTP requests and their responses. The first task, B6, successfully retrieves author names from a website and saves them as a JSON file. The next task, B7, involves downloading an image and resizing it. However, tasks B7 and B8 both failed. B8 specifically failed due to string formatting issues.
Same issue with my. I have built my docker image in mac air m1 but i found that my image was run on a x86_64 architecture (I can see this in the logs shared for x86_server_start.log)
@carlton sir i have same issue. I have built my docker image in mac air m1 but i found that my image was run on a x86_64 architecture.
Sir even my evaluation log file is missing and I really don’t know what to do because during submission 8/10 of my A tasks were working. Please look into it sir. This is really going to affect my grade and I remember how hard I tried just to get my A tasks running. Please sir Role nom 23f2000599 1000472083 1080×2400 255 KB
The image shows feedback on a Docker image evaluation, indicating potential issues. The evaluation either didn't run or the image was misconfigured, resulting in a score of 0 due to missing files. The image must become responsive within 5 minutes on an 8-core Xeon Google Cloud unit with fast network bandwidth. Seven files are described as essential for the process.
Hi @jivraj , The contents of Expected and Result matches, but still test case’s failed. Is there formatting check for answer , Isn’t prettier to be done ? I see that your expected answer isn’t formatted using prettier , am i wrong ? eg: EXPECTED: [{‘first_name’: ‘Kevin’, ‘last_name’: ‘Allen’, ‘email’: ‘tonya41@example.com’}, {‘first_name’: ‘Kimberly’, ‘last_name’: ‘Allison’, ‘email’: ‘vmendoza@example.com’}, {‘first_name’: ‘Kathleen’, ‘last_name’: ‘Baldwin’, ‘email’: ‘amclean@example.com’}, {‘first_name’: ‘Jason’, ‘last_name’: ‘Banks’, ‘email’: ‘sharptara@example.org’}, {‘first_name’: ‘Tami’, ‘last_name’: ‘Bass’, ‘email’: ‘kristy61@example.com’}, {‘first_name’: ‘Brenda’, … RESULT: [ { “first_name”: “Kevin”, “last_name”: “Allen”, “email”: “ tonya41@example.com ” }, { “first_name”: “Kimberly”, “last_name”: “Allison”, “email”: “ vmendoza@example.com ” }, { “first_name”: “Kathleen”, “last_name”: “Baldwin”, “email”: “ amclean@example.com ” }, { “first_name”: “Jason”, “last_name”: “Banks”, “email”: “ sharptara@example.org ” }, { “first_name”: “Tami”, “last_name”: “Bass”, “email”: “ kristy61@example.com ” }, { “first_name”: “Brenda”, “last_name”: “Bradford”, “email”: “ amandakeith@example.com ” },…
The image shows a yellow triangle with a thick, dark gray exclamation point inside. This is a common symbol used as a warning sign to alert people to potential hazards or important information requiring attention.

The image displays a yellow, glossy triangular warning sign. Inside the triangle is a large, dark gray exclamation point, indicating a general warning or alert to potential hazards or important information that requires attention.

Hi @all We will identify why arm images created a problem and were run using x86 platform. We will also rerun evaluations for all the x86 and arm images one more time, before pushing to the dashboard. 23f3003302: Hi @jivraj , @23f3003302 output from your server’s response is correct, we will update our evaluation script. 23f2004912: image 1460×585 24.9 KB @23f2004912 We will discuss internally if we can do something about it, but I can’t assure if you will get marks for it, since output from your server is a bit different. 23f1001611: image 2003×745 95 KB image2003×745 95 KB @23f1001611 we will look into it HarshJaiswal: This is the id of the docker image that was evaluated: d0f14a872042 , but i had never provided this docker image then how it get evaluated, also none of the docker image created by me has this id. @HarshJaiswal I looked for your response for project1 docker image, and found out that we used correct image id. Here is repo information harshjaiswal1/tds_project_final      latest    d0f14a872042   5 weeks ago    214MB @AYUSH_SINGH AYUSH_SINGH: ayush6871/fastapi-agent latest 27e8375b0ab1 6 weeks ago 1.66GB This was submitted to us through google form, for project1. AYUSH_SINGH: The 2 other log files i’m given doesnt have my email inside it listed. We are aware about it results for 12 students are not generated, we will look into it, and see what caused those 12 images not to run. @22f1000703 22f1000703: My evaluation log file is missing in report provided. It says tasksA was not found. but I have submitted tasksA in my project file. Also it says server didnt start for 5 mins but for me image was working fine. please kindly help me out. I have made submissions correctly. It would have run at your end but it was supposed to run at anywhere, after dockerising it didn’t run, reason is taskA module was not found.
The image features a blurred, white letter "H" against a solid teal or turquoise background. The letter appears to have a soft, glowing effect, possibly due to a shallow depth of field or blurring technique. The overall impression is simple and minimalist.

The image features a figure silhouetted against a vibrant, otherworldly scene. Above the figure is the celestial body casting light on the silhouetted figure. The sky is a mix of blues, purples, and reds, creating a cosmic and dreamy ambiance.

The image shows a test failure when comparing scraped data from `/data/b6.json` against expected values. The expected list of authors does not match the actual scraped list, which includes some differences in the order, encoding of "André Gide", and extra "Albert Einstein" entries. Consequently, the test "B6" failed.
The image shows a white, slightly blurred, capital letter "A". It is centered on a solid purple background. The letter has a soft glow effect around its edges, making it stand out against the vibrant background.

The image displays the results of a series of tasks likely executed in a programming environment. The first task, related to scraping author names from a website, succeeds.  A subsequent task involves downloading and resizing an image, but it fails, indicated by "B7 failed" and "B8 failed" with an error regarding string formatting.
The image features the letter "H" in white, presented with a soft, glowing effect. It's centered against a solid, light blue background. The letter appears to be slightly blurred, adding to the overall soft and ethereal aesthetic of the image.

The image shows a white, outlined letter "A" against a bright orange background. The letter appears slightly blurred or out of focus, giving it a soft, glowing effect. The composition is simple, highlighting the letter as the central and sole subject.

The image features a stylized letter "A" in white with a blurred, glowing effect. The letter stands out against a solid orange background. The overall impression is clean and minimalist, focusing on the letter's form and the contrast between the bright colors.

A man with short, dark hair and a mustache is wearing sunglasses and a light-colored shirt with a black undershirt. He appears to be indoors, possibly in a well-lit public space, with a display screen visible in the background. The lighting creates a warm tone in the image.

Same issue for me sir. When I evaluated my file using evaluate.py my 9 cases out of the 10 in Task A was passed but the email I received shows that my evaluation log file is missing. I don’t understand why does it show like that. Please do check and help me out sir. Reg no. 24f1002633
I suspect there is something wrong with how the evaluation has been done. Although A1 task succeeded, all of my A tasks failed.
I have checked my log file in all of the cases where a file is required it says file not found or directory not found error in the code, how can I check /data folder was provided to the program? @carlton
@Jivraj , @carlton It was a good project, and I have obtained the log files. Upon reviewing the log files, I realized that they are unable to read the files. I checked my project on GitHub and discovered that I forgot to uncomment the line that defines the path using the os library. As a result, all file evaluations returned errors such as “can’t read the file.” I understand that this oversight was my mistake. However, is there any way to reevaluate the code by simply uncommenting that line? I believe the rest of the code is properly written, but due to this single comment, all the files remained unchecked or resulted in errors. Screenshot (177) 1920×1080 206 KB Screenshot (179) 1920×1080 199 KB
The log file from a Google Drive document shows a series of HTTP requests and errors. It details failed attempts to read files such as 'mail.txt', 'mail-sender.txt', and 'card.jpg'. There are also errors during tasks involving extracting credit card numbers and finding similar comments using embeddings, indicated by HTTP 500 and 404 status codes.
The image shows a code editor displaying the "app.py" file within a GitHub repository for an LLM-based Automation Agent. The code includes API endpoints ("/read", "/run", and "/") and function definitions such as `read_file`, `run_task`, and `start`. The symbol sidebar indicates the presence of AI proxy variables and functions for tasks and data management.
Same here. I also dis not recieve any mail sir.
I noticed that my Docker image was run on an x86_64 architecture (as indicated by my email in the shared logs), whereas I originally built it on my Mac (ARM architecture). Due to this mismatch, the image failed to run properly and resulted in an exec format error. Since there was no prior mention of the architecture on which our images would be evaluated, I request that my evaluation be conducted again on the appropriate machine. Please help as after doing it correctly getting 0 marks because of such an error feels wrong
@23f2001975 we had to rely on docker telling us whether an image was arm or x86. So thats why we just did what docker software told us. If it classified an image as arm then we ran it on arm. If it did not then we ran it on x86. Thats why we need students to look through the logs and identify issues so that we can make sure you get the correct evaluation. If students notify us their image is actually arm based, then we will run it on arm. So dont worry, just inform us of any discrepancy as well as bugs. Our evaluation might not be perfect, there may be bugs. If students can precisely create bug reports then we can take that into consideration when evaluating students as well. The benefit being you might get extra marks because of the bug fix. We have a script that looks at this discourse post each day and tells us who requires a fresh evaluation. So we will check your image on arm. Kind regards
image 633×197 4.25 KB This is a screenshot of my docker log file. This works if you pass the actual value of the airproxy token at the command line while pulling the docker image. Please do look into this as I’ve put a lot of effort into this. Thank you Regards, 23f3002677
The image shows a Python traceback indicating a `KeyError: 'AIRPROXY_TOKEN'`. The error originates from line 30 in `/app/app.py`, where the code attempts to access the environment variable `AIRPROXY_TOKEN` using `os.environ['AIRPROXY_TOKEN']`. The traceback reveals that the environment variable is not set, resulting in the `KeyError`.

@cartlon Same issue. My image was also run on a x86_64 architecture. I too built on my mac which is ARM (M1 Processor). I too can see that my docker image never ran properly and threw the exec format error and Evaluation log file is MISSING. Can you please rerun the image on ARM based
