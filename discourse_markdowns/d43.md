The GA portal marks it as wrong unless I manually insert new lines. I hope that won’t be an issue, will it?
Your markdown must have newline characters or spaces wherever necessary. Otherwise we will not be able to check if your answer is correct. Our parser will only work if encodings for the formatting are present in the response. If there are no encodings (invisible or visible) then we will not have the correctly formatted file. Please review module 1 for a better understanding about how text is encoded. Especially invisible characters. The browser is designed for user friendliness. Thats why the characters are invisible when you copy paste string with newlines. But it exists. The programmatic strings show invisible encodings as escaped characters. (Usually) To check if a string has invisible characters, # Multi-line string
my_string = """Hello
World    with    spaces 
and some newlines
and a tab	"""

# Print ASCII values of each character
print([ord(c) for c in my_string]) e.g., newline = 10, tab = 9 This is a great way to check what we are receiving when you send us some response, import requests
import json

# This is just an example server to see what we see.

url = "https://httpbin.org/post"

my_multiline_string_answer = """This is a multiline
string that spans
multiple lines    with    spaces 
and some newlines
and a tab	as well."""

response_to_send_to_tds_evaluator = {
"answer": my_multiline_string_answer
}

# Send the JSON data
response = requests.post (url, json=response_to_send_to_tds_evaluator)

# Check the response
print (response.status_code)
print (response.json ())
print (response.text)

# Do other checks as necessary... See what happens when I print the result print (json.loads (response.text)['json']['answer']) Screenshot 2025-03-27 at 1.09.56 pm 323×120 3.61 KB Its a proper multiline correctly formatted text! The encodings are invisible just like in the original as well as in your clipboard when you copy paste into the GA. Here is a json example: json_answer = {
    "mary": "poppins",
    "age": 42
}

stringed_json = json.dumps (json_answer)
response_to_send_to_tds_evaluator = {
"answer": stringed_json
}

response = requests.post (url, json=response_to_send_to_tds_evaluator)

print (json.loads (response.text)['json']['answer']) Look at the response. A perfect json. If you do not want spaces in the response then strip the spaces before you send the stringified response. Kind regards
The image shows a terminal output. The user `venvcarland` on `Carlton-MacBook-Pro` executed the python script `test.py` located in the `/User/tds/` directory. The script prints a multiline string that spans across multiple lines, demonstrating the use of spaces, newlines, and tabs within the string.
The image displays a terminal session on a MacBook Pro. The user, "venvcarland", is in the "tds" directory and has executed the "test.py" script. The output of the script is a JSON-formatted string containing the name "mary" mapped to "poppins", and the key "age" mapped to the value 42.
I would request the TDS team to please consider making the evaluation criteria for project 2 a bit more liberal. 5 random Questions with 4 marks each is quite harsh. I request you to make it more balanced like 2 Qs from each GA with 2 marks each or so. So that even if we can’t exhaustively cover the whole number of questions, it gives mercy for the partial completion. 5 random question is a kind of lottery or luck based unless our app is perfect. Thanks @carlton @s.anand
Hey guys, I have designed the code, and it currently works for the first 11 GA1 questions and the sixth and third questions of GA2. I have accounted for all edge cases. Please check the code at this link: https://4e52-43-230-106-58.ngrok-free.app/ . Let me know your thoughts!
@carlton , @Jivraj @Saransh_Saini GA2 - Question 3: Publish a Page Using GitHub Pages As part of the requirement, I successfully published a webpage using GitHub Pages that includes my email address 21f3001076@ds.study.iitm.ac.in in the HTML content. The page functions correctly and becomes accessible on my local system. To automate the publishing process, I implemented a delay function that checks for the page’s availability after 5 seconds. Based on testing, GitHub Pages typically take around 10–20 seconds to go live after repository creation and HTML deployment. As a result, the complete process—from initiating the API call to verifying that the page is live—takes approximately 30 seconds locally. This setup works reliably on my local machine. However, when deploying the same process on Azure, I encountered an issue. Without the delay, the API responds too quickly—before the GitHub Pages site is actually live—resulting in a broken or non-functional link on the assignment portal. On the other hand, including the delay function causes Azure to throw a 502 Bad Gateway error, likely due to Azure’s request timeout limitations. The additional wait time slightly exceeds the platform’s allowed response duration. GA4 - Question 9: Process PDF Files A similar issue occurs in GA4 Question 9, where the task involves processing PDF files. While this works perfectly in the local environment, it leads to a 502 Bad Gateway error on Azure. This is due to the relatively long time required to parse and analyze the PDFs, which again exceeds Azure’s execution time limit. Moreover, pre-processing the PDF files is not an option because the input varies for each user. Therefore, the PDFs must be processed dynamically, which adds to the delay and contributes to the timeout problem. Currently, I am using Azure for deployment, and for the majority of tasks, it works reliably. Although these specific tasks face timeout issues, shifting to another deployment platform is not feasible at this point. I am not certain if alternative platforms will work consistently across all questions, and making such a change could introduce failures in other parts of the assignment where Azure performs well. Below Image is showing response of local machine api request for GA2 Q3 which works fine. image 1854×493 55.9 KB
The image shows a web request and its response, indicating a successful (code 200) retrieval of data from a server. The response body contains a URL for a Github page. It also shows tips for what the Github pages URL might look like and how to bust the cache.
@s.anand @carlton @Jivraj @Saransh_Saini Final Thoughts on Project 2 Project 2 has been a valuable yet challenging experience. I’d like to share a few reflections—not as criticism, but as constructive feedback with the intention to contribute positively to future iterations. Organization & Structure: The project felt a bit unstructured at times, which may be expected given that this is possibly the first time such an initiative is being carried out. I’m not blaming anyone—just sharing my thoughts. I hope positive insights can be drawn from this feedback. Deployment & Portal Limitations: There are noticeable limitations on the deployment side (e.g., timeouts, platform constraints) and occasional issues on the assignment portal. While frustrating, these also led to some deep technical learning. Time Constraints: Balancing this project with ongoing coursework and other responsibilities has been quite demanding. The deadline of March 31st adds pressure, making it difficult to thoroughly test all 57 questions across different environments. Sessions & Support: The sessions conducted were helpful in addressing specific topics, but in many areas, I had to rely heavily on GPT and self-guided exploration to move forward. Deployment Variability: Each GA’s questions seemed to expose different limitations depending on the deployment platform. This inconsistency made it hard to predict which questions would work reliably. Effort Acknowledgment: I sincerely appreciate the efforts of the TAs and instructors—it’s clear that a lot of hard work went into enabling and supporting this project. However, it’s also a bit disappointing that we, as students, are unsure which questions are expected to work and which may fail—even the team might not have full visibility over this at the moment. Feasibility in the Given Timeline: Even for someone with a technical background, it’s difficult to troubleshoot and experiment across so many cases within the current timeline. Trying different approaches per question isn’t always feasible before the deadline. I genuinely hope someone is able to complete all 57 questions successfully—I’d love to learn from their experience and solutions. Again, this post is meant in a positive and constructive spirit. If anything I said seems inappropriate or off the mark, please let me know—I’m happy to edit or delete the post if needed. Thank you for the learning opportunity
I have implemented till GA 4 q8 with few questions here and there like one with the llamafile, etc. If someone have the functions of GA 5 ready please ping me for collaboration.
@carlton How to do the GitHub Actions questions as it is a cron job based but need manual trigger for the eval script to verify. If done without cron job the portal complains.
If there is any plan for extension, I request the TDS team to announce it in advance as people would rush and submit, then, they come hear about the extension when they had submitted it already. Happened to me like almost every time. We have OPPE this weekend, so if there is slightest possibility, I request to announce it in advance. Thanks
@Saransh_Saini @carlton Can someone reply to this?
The image shows a yellow circular emoji face with a wide, open smile revealing the teeth. Above the smile, the eyes are closed with a slight curve to indicate happiness or amusement. A single drop of blue sweat is visible on the upper-right side of the face, suggesting a nervous or awkward situation.
WhatsApp Image 2025-03-27 at 13.41.43_5bd0a182 1600×1069 246 KB Dear Sirs, I have a question regarding Q3 and Q4 of GA5. When calling the API, should we pass the .gz file directly, or will the API accept a Google Drive link from which it can download the .gz file? Specifically, will the API call be structured as follows? Essentialy, will the API call look like so? curl -X POST “ http://127.0.0.1:5000 ” -H “Content-Type: multipart/form-data” -F "question=Bandwidth Analysis for Regional Contents - anand.net is a personal website that had region-specific music content. One of the site’s key sections is tamilmp3, which hosts music files and is especially popular among the local audience. The website is powered by robust Apache web servers that record detailed access logs. These logs are essential for understanding user behavior, server load, and content engagement. By analyzing the server’s Apache log file, the author can identify heavy users and take measures to manage bandwidth, improve site performance, or even investigate potential abuse. Your Task: This GZipped Apache log file has 258,074 rows. Each row is an Apache web log entry for the site s-anand.net in May 2024. Each row has these fields: IP: The IP address of the visitor. Remote logname: The remote logname of the visitor. Typically ‘-’. Remote user: The remote user of the visitor. Typically ‘-’. Time: The time of the visit. E.g. [01/May/2024:00:00:00 +0000]. Note that this is not quoted, and you need to handle this. Request: The request made by the visitor. E.g. GET /blog/ HTTP/1.1. It has three space-separated parts: (a) Method: The HTTP method. E.g. GET. (b) URL: The URL visited. E.g. /blog/. (c) Protocol: The HTTP protocol. E.g. HTTP/1.1. Status: The HTTP status code. If 200 <= Status < 300, it is a successful request. Size: The size of the response in bytes. E.g. 1234. Referer: The referer URL. E.g. https://s-anand.net/ . User agent: The browser used. This will contain spaces and might have escaped quotes. Vhost: The virtual host. E.g. s-anand.net . Server: The IP address of the server. The fields are separated by spaces and quoted by double quotes (‘-’). Unlike CSV files, quoted fields are escaped via \" and not ‘-’. (This impacts 41 rows.) All data is in the GMT-0500 timezone, and the questions are based on this same timezone. Filter the Log Entries: Extract only the requests where the URL starts with /tamilmp3/. Include only those requests made on the specified 2024-05-23. Aggregate Data by IP: Sum the ‘Size’ field for each unique IP address from the filtered entries. Identify the Top Data Consumer: Determine the IP address that has the highest total downloaded bytes. Report the total number of bytes that this IP address downloaded. Across all requests under tamilmp3/ on 2024-05-23, how many bytes did the top IP address (by volume of downloads) download?" -F “file=@s-anand.net-May-2024.gz” I would appreciate your clarification on whether the .gz file should be directly included in the API request or if a Google Drive link should be provided instead. Thank you for your time and assistance.
A 61MB GZipped Apache log file from s-anand.net in May 2024 needs processing.  Each log entry has fields like IP, time, request, status, and size, separated by spaces and escaped double quotes.  The task is to filter entries with URLs starting with `/malayalammp3/` on 2024-05-15 and then aggregate the "Size" field by unique IP addresses.
Hello Everyone i have cread solution now just for graded 1 and graded 2 a240-43-230-106-58.ngrok-free.app TDS - Tools for Data Science ask question there is only one problem in one question like in one question  my code unable to match desired function so giving other function output
The endpoint a240-43-230-106-58.ngrok-free.app is offline.
Now Online check it is online
so I have a doubt whatever output my code gives for the ga question , if i copy it one to one and paste in the ga page and it says correct .Would it mean the same would happen with the evaluation method ? or is there any extra things i must add? { “answer” : “answer inside string” }
lakshaygarg654: GA4 - Question 9: Process PDF Files A similar issue occurs in GA4 Question 9, where the task involves processing PDF files. While this works perfectly in the local environment, it leads to a 502 Bad Gateway error on Azure. This is due to the relatively long time required to parse and analyze the PDFs, which again exceeds Azure’s execution time limit. Moreover, pre-processing the PDF files is not an option because the input varies for each user. Therefore, the PDFs must be processed dynamically, which adds to the delay and contributes to the timeout problem. @carlton I watched the last session. In that session, regarding the specific PDF question, you mentioned that the PDF is the same for everyone, so it can be preprocessed beforehand. However, I checked and found that the PDF is actually different for each user. So, we need to fetch it from the API endpoint. How should we handle the timeout issue on the deployment platform? I even tried upgrading the plan, but it didn’t help. @s.anand @carlton @Jivraj Also, many questions and doubts were addressed in the last two sessions. I can improve a lot and add the remaining questions, but the constraint is the 31st March deadline. Most students, including myself, will only get time after 30th March due to Viva and OPPE. It would be really helpful if the TDS team could extend the deadline. I believe it would strike a good balance—team made us wait for the Project 1 results, but extending the Project 2 deadline would make up for that for some extent. Its request nothing else.
Here is a summary of the image:

The image shows a portrait of a man wearing a black suit jacket over a white shirt. He has short dark hair and a neatly trimmed beard. The background is a blurred light color. His expression is neutral and he is looking directly at the camera.
Hello sir, My question is, the questions my endpoint will be evaluated are they the exact same ones with same parameters for my respective email as on the assignment portal ? Essentially I am trying to understand there are certain questions where I don’t have to solve the question again and just return the hardcoded answers, would that be correct ?
Anyone please respond and also for ga 1 q 8 for instance where we have: Let’s make sure you know how to use JSON. Sort this JSON array of objects by the value of the age field. In case of a tie, sort by the name field. Paste the resulting JSON below without any spaces or newlines. [{"name":"Alice","age":67},{"name":"Bob","age":53},{"name":"Charlie","age":34},{"name":"David","age":89},{"name":"Emma","age":92},{"name":"Frank","age":37},{"name":"Grace","age":4},{"name":"Henry","age":49},{"name":"Ivy","age":30},{"name":"Jack","age":2},{"name":"Karen","age":2},{"name":"Liam","age":5},{"name":"Mary","age":32},{"name":"Nora","age":56},{"name":"Oscar","age":19},{"name":"Paul","age":22}] Sorted JSON: here would question be sent as text only or text + json file seperately?
Can Somebody please share the answers for GA 3 and 4, as I have missed those? Thank you
@carlton @Jivraj @Saransh_Saini
