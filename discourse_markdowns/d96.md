No. Because someone else might have another minor issue they want to fix. We have to apply the rule uniformly.
Ok… I do have a doubt tho, i actually have app.py and main.py in my github, my main.py is running on 8000 and app.py on 5000 …
but in Dockerfile in your github repo you didn’t run main.py,
In your Dockerfile you didn’t copy taskA.py to the container.
Ouch, ok sir… hopefully project 2 doesnt disappoint
Error: Failed to process image URL 'https://emoji.discourse-cdn.com/google/sob.png?v=14' after 5 retries. Last error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyDqHqkYWWr_VlNmsL-SYK4wKl4tPElJmhw
It is there in the master branch of the repository. Now, will the previous evaluation mail that we got be considered or this one?
@carlton @Jivraj I recently received an email with an evaluation file for Project 1, which included a score. However, in the recent email, I noticed that my score was recorded as zero, despite fulfilling all the prerequisites. I kindly request a re-evaluation of my project, as I believe this may be an error.
@Jivraj My discrepancy is still not fixed. Please take a look at it
@Jivraj Hlo, could you please check and let me know how much am I scoring in Project 1 after the latest evaluation?
@Jivraj @carlton Sir, In the mail that i got about project 1 report. In the log file it was written as TasksA.py file not found in docker, which was the case i observed with many students. Screenshot 2025-04-04 at 10.31.02 AM 1358×906 47.7 KB This is my Github repo: Screenshot 2025-04-04 at 10.44.24 AM 3324×1794 497 KB I built the image using docker build command in vs code terminal. And pushed it same way to dockerhub using docker push command. How is it possible that the docker container missed the TasksA.py file while building or pushing it? After getting this mail, I ran the project locally again mutliple times just to check if there was any issues in the code. It was getting 9/10 test cases passed.
The image shows a console output of installing Python packages. It downloads and builds packages like scipy, pandas, numpy, pydantic-core, duckdb, antiorm, db, and db-sqlite3. After installing 33 packages in 56ms, a `ModuleNotFoundError` occurs when trying to import `tasksA` in `/app/app.py`.
The GitHub repository "tds-project1" by GaURaVinDeX shows a Python-based project.  It contains Python scripts (app.py, tasksA.py, tasksB.py), a requirements file, a Dockerfile, and an MIT license. The initial commit was two months ago. A README file is missing, and the repository has one watcher but no stars or forks.  Python makes up 98% of the code.
This is a common mistake many, many students made. They created a working application but not a working container. Screenshot 2025-04-04 at 11.13.32 am 2116×1512 323 KB You only copied app.py into your docker image. How do you expect your application to run without the other files that your repo clearly shows is needed? Thats why many people are failing this. Hope the image makes this clear. Kind regards
This Dockerfile configures an environment for a Python application using FastAPI and Uvicorn. It starts from a Python 3.12 base image, installs necessary dependencies like `uv`, FastAPI, and Uvicorn. Then, it sets the working directory to `/app`, copies the `app.py` file into it, and defines the command to run the application.
1000050348 1080×2340 154 KB 1000050349 1080×2340 190 KB I am getting license not present at root of github repo but i have the license added could someone please explain why ?
The email informs the learner about the prerequisite checks for Project 1, referencing a detailed evaluation page. It outlines five requirements related to GitHub repository accessibility, MIT license, Dockerfile validity, Docker image accessibility, and Dockerfile consistency. The learner's evaluation shows that while Docker image, GitHub repo, and Dockerfile are present and public, the MIT license is missing, resulting in a failing grade and a Project 1 score of 0.
Error: Failed to process image URL 'https://europe1.discourse-cdn.com/flex013/uploads/iitm/optimized/3X/0/3/031fb8ca7808375905f4725bba8fa6e38751802d_2_230x500.jpeg' after 5 retries. Last error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyDqHqkYWWr_VlNmsL-SYK4wKl4tPElJmhw
@thinkmachine Firstly, you have passed evaluation and got a decent score (on a more lenient script that we used for everyone.) The email was sent by a script that used a more stricter evaluation (which understandably caused some stress). So you can breathe a sigh of relief. However with regards to your long post… Let me tell you a true story. I personally know a very experienced senior engineer at a top defense contractor for the US, here is some pearls of wisdom from him. What you have done is what is called in industry as gold plating . Its a cardinal sin in software engineering. NEVER gold plate. ALWAYS build to spec. In fact its a good reason to fire an engineer. Why? Because it does not deliver what was required, Wastes valuable time and resources Increases technical debt (this is actually a huge cost over the expected lifetime of the project!) Complicates testing Leads to scope creep His advice to me was simple: NEVER gold plate. I hope you take this pearl of wisdom in your career. It will help you advance and make you stand out. For personal hobbies this does not apply. But for a client (including us) if you fail to deliver the minimum spec then we cannot grant you an evaluation (by the way this post is not targetted specifically for you, it just felt like an appropriate place to explain this). Kindest regards
Error: Failed to process image URL 'https://emoji.discourse-cdn.com/google/slight_smile.png?v=14' after 5 retries. Last error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyDqHqkYWWr_VlNmsL-SYK4wKl4tPElJmhw
Hi Sir, I just realized that I mistakenly submitted the image tag “abhay227/version1” instead of the correct image ID. The correct image ID is 4db729a03f74 , which is part of version1 that is already present and publicly available. I have worked very hard on this project, and I am concerned that due to this error, my whole effort may be wasted. Unfortunately, I did not receive any notification regarding an invalid submission after I submitted the Project1 form, and I only recently became aware of this mistake. I kindly request you please consider this correct image ID. Thank you for your understanding and assistance. I look forward to your positive response. @carlton @Jivraj Screenshot 2025-04-02 132214 1843×250 18.1 KB
Error: Failed to process image URL 'https://europe1.discourse-cdn.com/flex013/uploads/iitm/optimized/3X/6/d/6d91cbdb81d6b92d0da76715ba6725eb015b11d1_2_690x93.png' after 5 retries. Last error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyDqHqkYWWr_VlNmsL-SYK4wKl4tPElJmhw
Hi Abhay, This was a basic error. Unfortunately for basic errors we are not able to relax the requirements. All students were given a clear directive on what the minimum requirements were in order to be evaluated. Failure to follow those clear instructions prevents us from making any exceptions, because then we just have to dump all those requirements for all students and that would not be fair to those that took the care to be careful about their submissions. Kind regards
Hi sir, hope you are doing well. Could you please check and let me know if I have passed the project 1 and if yes then how much am I scoring in Project 1 after the latest evaluation? @carlton
Thanks for the clarification regarding the evaluation, @carlton . It’s a relief to know that my original submission was successfully evaluated. Had I known that the stricter evaluation script would pull the image matching the digest from the time of submission (which had been overwritten by April 1), I would’ve used a separate tag to avoid the issue altogether. Regarding your point on gold plating — I completely understand and have come to appreciate the importance of building to spec from personal experience, especially in production or client-facing settings where fixed targets, maintainability, and minimal scope creep are key. That said, with TDS projects, my goal was purely exploratory — to explore the boundaries of what’s possible within the scope of the problem statement . What began as just another (pun intended) tedious assignment slowly evolved into a hobbyist research project on LLM agents. (…caution: long post ahead ) I noticed that test cases in Project 1 and 2 were highly specific and often overlapping on Python & Shell use . While it would’ve been easy to hardcode 50+ Python functions to pass these cases (which, frankly, many of us were doing), it is non-scalable at best. I quickly realized that stochastic parrots + hardcoded functions were a recipe for disaster, especially considering the inherent randomness in LLM-generated payloads . No two payloads are exactly alike — even minor differences, like an absolute vs relative file path, or some hidden edge case could cause a hardcoded solution to fail unpredictably. There’s also really no way to predict an edge case caused by an LLM. Some might suggest using temperature=0 to get more deterministic LLM behavior — and while true to an extent, it does little to encourage exploration, especially in tasks that require self-correction based on environmental feedback. Prompt engineering too wouldn’t be helpful here as 4o-mini isn’t all that great at 0-shot instruction following, especially when the system prompt is already saturated with 50+ fine-grained instructions. There’s only so much stuff it can pay attention to. Hardcoded tool agents also aren’t really “agents” in my view— they’re more like passive AI powered regex matchers : merely mapping inputs to functions by inferring from context window. That puts all the burden of answering on the hardcoded functions, leaving the agent itself uninvolved in the solutioning process. If they break, the agent will never try to ‘fix’ them and keep calling them like a broken record. At the core of it, it’s all about how much flexibility vs rigidity we give to the agent. Fully rigid solutions (e.g. hardcoding) overfit and break easily; fully flexible ones (e.g. pure LLM based) hallucinate or drift off-target. The sweet spot lies somewhere in between — The right solution would naturally balance the lesser of two evils in an ideal ratio. Since most LLMs already excel at code generation and structured solutioning, the most effective strategy that I figured out for the agent was to, Reason about the task, understand intent, Reflect, whether this problem is solvable using available tools without human intervention and design structured workflows, in whatever order appropriate (i.e. design a DAG, where each node can be a python step or a shell step or something else) Execute those workflows ( walk the DAG) observing the feedback at each step and reiterating if needed. Observe the final result, and repeat if needed. Interestingly, a similar framework was suggested in this ICLR 2022 paper . That was all the validation I needed to know I was stepping in the right direction. To make environment interaction possible, the agent didn’t need dozens of narrow tools — just a small, well-defined set of general-purpose tools : A REPL executor (for quick calcs) A Python script runner A Shell executor With just these, it could handle most tasks flexibly and naturally — avoiding overengineering while still enabling powerful behaviors. Potentially allowing for full fledged Computer-Use via shell and so much more. As for the fact that it ended up being capable of things beyond the scope of Project 1 (like training & tuning ML models autonomously, reporting results etc.) — that was emergent behavior , not deliberate gold plating. It was a pleasant surprise even to me. I’ve yet to discover more of such interesting hidden use cases. While some might naturally call it scope creeping (and yes that is true, given that we had a deadline, and a play-pretend client-business relationship with the course team), I saw it as an opportunity for exploration and research. Frankly, I AM personally very keen about researching stuff! I am actually very thankful to the TDS course team & @s.anand for devising such a thoughtful project that sparked some interesting ideas that I can tinker with. Food for thought! Really! As for my next project, I now have a fair idea of what I’ll be experimenting with— modalities. PS: I’m not claiming it’s perfect or production-ready, or it should score a perfect 22/20, but it aligned well with what I believe was the spirit of these projects: thoughtful use of LLMs in agent design . At this point, I’m less concerned about the marks, I’m actually enjoying the thought joyride. TL;DR My approach doesn’t rely on regex or hardcoded mappings. Instead, it passes user input directly to an LLM, which then plans and executes workflows using general tools inside a containerized environment. It also learns from feedback and iterates — much like a human. The fact that it can do more than just the minimum spec is a byproduct of that framework. I’ve only just wired the pieces together. Kind regards
Error: Failed to process image URL 'https://emoji.discourse-cdn.com/google/grinning_face_with_smiling_eyes.png?v=14' after 5 retries. Last error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyDqHqkYWWr_VlNmsL-SYK4wKl4tPElJmhw
Error: Failed to process image URL 'https://emoji.discourse-cdn.com/google/sweat_smile.png?v=14' after 5 retries. Last error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyDqHqkYWWr_VlNmsL-SYK4wKl4tPElJmhw
The image features a classic, bright yellow emoji. It's a round face with a wide, open mouth showing the upper teeth and tongue. The eyes are closed in a joyful, expressive squint, conveying genuine laughter and happiness.

@carlton @Jivraj Sir please Consider this request!
Hello Sir, Screenshot_2025-04-05-18-51-43-721_com.google.android.gm 1080×2400 144 KB I got this mail regarding my project 1 scores. My github repo is present and public as well as MIT License and Dockerfile  is also present at the root of the repo github.com GitHub - SrishtySnehi/Project_1_tds Contribute to SrishtySnehi/Project_1_tds development by creating an account on GitHub.
The image shows an email with project 1 prerequisite evaluations. The Docker image is public and present, but the Github repository is either not present or not public. Both the Dockerfile and MIT license are missing from the root of the Github repository, so the prerequisites and overall project score are failing.
The image displays the GitHub repository "SrishtySnehi/Project_1_tds," with a graphic of a pixelated pink cross on a white square. It shows one contributor, zero issues, stars, and forks, indicating a relatively new or unmaintained project.
Hi @Srishty_Snehi Your submission is valid, we but it failed while running server, with this error. taskA module missing For regenerating this error: Pull github repo(latest commit before 18th Feb) Build image using Dockerfile of fetched repo Run that image.
